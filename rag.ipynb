{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6e6dab",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b6dc3e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: langchain-ollama in ./.venv/lib/python3.12/site-packages (0.3.6)\n",
      "Requirement already satisfied: ollama in ./.venv/lib/python3.12/site-packages (0.5.3)\n",
      "Requirement already satisfied: langchainhub in ./.venv/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.12/site-packages (1.0.17)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: bs4 in ./.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Collecting langchain_chroma\n",
      "  Downloading langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.3.74)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.3.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: httpx>=0.27 in ./.venv/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in ./.venv/lib/python3.12/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (2.32.4.20250809)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.12/site-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.12/site-packages (from bs4) (4.13.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: langchain_chroma\n",
      "Successfully installed langchain_chroma-0.2.5\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-ollama ollama langchainhub chromadb langchain bs4 langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5bac35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9c2ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OLLAMA_HOST=http://172.25.208.1:11434\n"
     ]
    }
   ],
   "source": [
    "! source get_host.sh && echo $OLLAMA_HOST > tmp_env.txt\n",
    "with open('tmp_env.txt') as f:\n",
    "    OLLAMA_HOST = f.read().strip()\n",
    "%env OLLAMA_HOST=$OLLAMA_HOST\n",
    "import os\n",
    "os.remove(\"tmp_env.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b6a1f",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc52b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c34c579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name:str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dbaa2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text embedding models\n",
    "# https://ollama.com/blog/embedding-models\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# embd = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embd = OllamaEmbeddings(model=os.environ[\"OLLAMA_EMBED_MODEL\"])\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ec7c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.2761092814708602\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27edab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import os\n",
    "\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f0ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d65426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstores\n",
    "# index\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(model=os.environ[\"OLLAMA_LLM_MODEL\"])\n",
    "                                   )\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9bb737",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(model=os.environ[\"OLLAMA_EMBED_MODEL\"])\n",
    "                                   )\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2708360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98f31091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25a028",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88e72e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=os.environ.get(\"OLLAMA_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"OLLAMA_HOST\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "815a76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d593e36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition is the process of breaking down a complicated task into smaller and simpler steps. It involves using techniques such as Chain of Thought (CoT), Tree of Thoughts, or other methods to analyze the problem and generate multiple possible solutions at each step. The goal is to make the task more manageable and understandable for an agent (e.g., a language model) by transforming complex tasks into multiple smaller ones.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-08-26T18:10:17.0767458Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1250924700, 'load_duration': 58223700, 'prompt_eval_count': 332, 'prompt_eval_duration': 22225900, 'eval_count': 82, 'eval_duration': 1169818900, 'model_name': 'llama3.2:3b'}, id='run--90d69d2b-33ca-4052-95ef-166e61672b1d-0', usage_metadata={'input_tokens': 332, 'output_tokens': 82, 'total_tokens': 414})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke\n",
    "chain.invoke({\"context\": docs, \"question\": \"What is task decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c86b248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt hub\n",
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e98f61",
   "metadata": {},
   "source": [
    "Rag Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74309b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, Task Decomposition refers to breaking down complex tasks into smaller and simpler steps, allowing an agent (or a model) to plan ahead and tackle hard tasks more efficiently. This technique is also known as Chain of Thought (CoT), which involves instructing the model to \"think step by step\" to decompose the task.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404ae99",
   "metadata": {},
   "source": [
    "## Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc091f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# mutli-query\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help the user\n",
    "overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by only one newline character. Only provide the requested\n",
    "perspectives without any additional text. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOllama(temperature=0, model=os.environ[\"OLLAMA_LLM_MODEL\"])\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [line for line in x.split('\\n') if line.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc770e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2d0b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals. This process enables efficient handling of complex tasks.\\n\\nThere are three ways to perform task decomposition:\\n\\n1. Using simple prompting with instructions like \"Steps for XYZ. 1.\", or \"What are the subgoals for achieving XYZ?\".\\n2. Using task-specific instructions, such as \"Write a story outline.\" for writing a novel.\\n3. With human inputs.\\n\\nTask decomposition is used in conjunction with other techniques, such as Chain of Thought (CoT) and Tree of Thoughts (Yao et al. 2023), to help the LLM agent plan ahead and tackle complex tasks.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f49a7e",
   "metadata": {},
   "source": [
    "## RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f44dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search quesries based on a single input entry. \\n Output (4 queries:)\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82f49352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOllama(temperature=0, model=os.environ[\"OLLAMA_LLM_MODEL\"])\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [line for line in x.split('\\n') if line.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01ba27e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used inthe RRF formula \"\"\"\n",
    "    \n",
    "    # initialize a dictionary to hold fused scores for each unique deployment\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be seriealsed to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_score dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # retrieve the current score of the document\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "    \n",
    "    # Sort the docuents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f11a0824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, task decomposition for LLM (Large Language Model) agents involves breaking down large tasks into smaller, manageable subgoals. This allows the agent to efficiently handle complex tasks.\\n\\nThere are three ways to do task decomposition:\\n\\n1. Using simple prompting: e.g., \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\\n2. Using task-specific instructions: e.g., \"Write a story outline.\"\\n3. With human inputs.\\n\\nTask decomposition is one of the key components of a LLM-powered autonomous agent system, along with reflection and refinement.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2d41c",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b179b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Do not provide any other text, only provide the generated search queries \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d6837de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the role of a large language model in an autonomous agent system?',\n",
       " '2. How do reinforcement learning algorithms contribute to the development of autonomous agents powered by LLMs?',\n",
       " '3. What are the key differences between LLM-powered and traditional machine learning approaches for autonomous decision-making?']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() |  (lambda x: [line for line in x.split('\\n') if line.strip()]))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9636931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n -- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0220218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n --- \\nQuestion: 1. What is the role of a large language model in an autonomous agent system?\\nAnswer: Based on the provided context, a large language model (LLM) plays a crucial role as the brain of an autonomous agent system. In this setup, the LLM functions as the primary controller, complemented by several key components:\\n\\n1. Planning: The LLM breaks down complex tasks into smaller subgoals, enabling efficient handling of intricate tasks.\\n2. Reflection and refinement: The LLM can perform self-criticism and self-reflection over past actions, learning from mistakes and refining them for future steps to improve the quality of final results.\\n\\nIn summary, the role of a large language model in an autonomous agent system is to serve as the intelligent core controller that enables planning, reflection, and refinement, ultimately driving the agent's decision-making process.\\n --- \\nQuestion: 2. How do reinforcement learning algorithms contribute to the development of autonomous agents powered by LLMs?\\nAnswer: Based on the provided context, reinforcement learning algorithms play a crucial role in the development of autonomous agents powered by large language models (LLMs). Here's how:\\n\\n1. **Learning from interactions**: Reinforcement learning algorithms enable the agent to learn from its interactions with the environment, receiving rewards or penalties for its actions. This process helps the LLM refine its decision-making and improve its performance over time.\\n2. **Policy optimization**: Reinforcement learning algorithms can optimize the policy of the LLM, which is the mapping between states and actions. By iteratively updating the policy based on the agent's experiences, reinforcement learning algorithms help the LLM make more informed decisions in complex environments.\\n3. **Value function estimation**: Reinforcement learning algorithms also enable the estimation of value functions, which represent the expected utility or reward of taking a particular action in a given state. This information is crucial for the LLM to make informed decisions and balance competing goals.\\n4. **Exploration-exploitation trade-off**: Reinforcement learning algorithms help the agent navigate the exploration-exploitation trade-off, balancing the need to explore new actions and states with the need to exploit known opportunities for reward.\\n\\nIn the context of LLM-powered autonomous agents, reinforcement learning algorithms can be used to:\\n\\n* Train the LLM on tasks such as planning, reflection, and refinement\\n* Optimize the policy of the LLM to balance competing goals and optimize performance\\n* Estimate value functions to inform decision-making\\n* Enable the agent to learn from its interactions with the environment and adapt to changing conditions\\n\\nBy leveraging reinforcement learning algorithms, developers can create more sophisticated autonomous agents powered by LLMs, capable of learning from their environments and improving their performance over time.\\n --- \\nQuestion: 3. What are the key differences between LLM-powered and traditional machine learning approaches for autonomous decision-making?\\nAnswer: Based on the provided context and background information, here are the key differences between LLM-powered and traditional machine learning approaches for autonomous decision-making:\\n\\n**Key Differences:**\\n\\n1. **Planning and Subgoal Decomposition**: LLM-powered agents use planning as a key component to break down complex tasks into smaller subgoals, enabling efficient handling of intricate tasks. In contrast, traditional machine learning approaches may rely on more simplistic methods such as rule-based systems or heuristic search.\\n2. **Reflection and Refinement**: LLM-powered agents can perform self-criticism and self-reflection over past actions, learning from mistakes and refining them for future steps to improve the quality of final results. Traditional machine learning approaches typically do not have this level of self-awareness and reflection capabilities.\\n3. **Learning from Interactions**: LLM-powered agents learn from their interactions with the environment using reinforcement learning algorithms, which enable the agent to receive rewards or penalties for its actions. Traditional machine learning approaches may rely on supervised learning or unsupervised learning methods, which do not involve direct interaction with the environment.\\n4. **Policy Optimization and Value Function Estimation**: LLM-powered agents use reinforcement learning algorithms to optimize their policy and estimate value functions, which represent the expected utility or reward of taking a particular action in a given state. Traditional machine learning approaches may rely on more simplistic methods such as gradient descent optimization or heuristic search.\\n5. **Exploration-Exploitation Trade-off**: LLM-powered agents use reinforcement learning algorithms to navigate the exploration-exploitation trade-off, balancing the need to explore new actions and states with the need to exploit known opportunities for reward. Traditional machine learning approaches may not have this level of nuance in their decision-making processes.\\n\\n**Traditional Machine Learning Approaches:**\\n\\nIn contrast, traditional machine learning approaches for autonomous decision-making often rely on more simplistic methods such as:\\n\\n* Rule-based systems\\n* Heuristic search\\n* Supervised learning\\n* Unsupervised learning\\n* Gradient descent optimization\\n\\nThese approaches may not have the same level of self-awareness, reflection capabilities, or ability to learn from interactions with the environment as LLM-powered agents.\\n\\nIn summary, LLM-powered autonomous decision-making approaches offer several key advantages over traditional machine learning approaches, including planning and subgoal decomposition, reflection and refinement, learning from interactions, policy optimization and value function estimation, and exploration-exploitation trade-off.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm \n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "\n",
    "# Generate QA pairs\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pair\n",
    "\n",
    "q_a_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b193e",
   "metadata": {},
   "source": [
    "## Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e7dbc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Could the members of the police perform lawful arrests?', 'output': 'What can the members of the police do?'}, {'input': \"Jan Sindel's was born in what country?\", 'output': \"What is Jan Sindel's personal history?\"}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of the police perform lawful arrests?\",\n",
    "        \"output\": \"What can the members of the police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"What is Jan Sindel's personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to examples messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. You must respond with only one step-back question with no added text. Here are a few examples:\"\"\"\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "20c5ff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do large language models process complex tasks?'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL']) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "62e58dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial component of LLM-powered autonomous agent systems, enabling the efficient handling of complex tasks by breaking them down into smaller, manageable subgoals. In the context of LLM agents, task decomposition involves instructing the model to \"think step by step\" and utilize more test-time computation to decompose hard tasks into simpler steps.\\n\\nThere are several techniques used for task decomposition in LLM-powered agent systems:\\n\\n1. **Chain of Thought (CoT)**: This technique, introduced by Wei et al. (2022), involves instructing the model to \"think step by step\" and generate a chain of thoughts that decompose the complex task into smaller steps. CoT transforms big tasks into multiple manageable tasks and sheds light on the model\\'s thinking process.\\n2. **Tree of Thoughts (ToT)**: This technique, introduced by Yao et al. (2023), extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be performed using breadth-first search (BFS) or depth-first search (DFS) with each state evaluated by a classifier or majority vote.\\n3. **Simple prompting**: LLMs can also perform task decomposition through simple prompting, such as \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\".\\n4. **Task-specific instructions**: Task-specific instructions, such as \"Write a story outline,\" can be used to guide the model\\'s task decomposition process.\\n5. **Human inputs**: Human inputs can also be used to provide guidance on task decomposition, allowing the model to learn from human feedback and improve its performance over time.\\n\\nThe goal of task decomposition in LLM-powered agent systems is to enable the model to break down complex tasks into smaller, more manageable subgoals, which can then be tackled one by one. This approach allows the model to efficiently handle complex tasks and improve the quality of its final results through self-reflection and refinement.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "# Response prompt\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0da1d",
   "metadata": {},
   "source": [
    "## HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7837812a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for Large Language Model (LLM) agents refers to the process of breaking down complex tasks into smaller, more manageable sub-tasks that can be executed by the model in a hierarchical manner. This approach enables the development of more robust and generalizable LLMs by allowing them to focus on specific aspects of the task at hand, rather than attempting to tackle the entire task simultaneously.\\n\\nBy decomposing tasks into sub-tasks, LLM agents can leverage their strengths in processing and generating text to address specific components of the task, such as question answering, text classification, or language translation. This hierarchical approach also enables the model to learn more nuanced representations of the task and its sub-tasks, leading to improved performance and generalizability.\\n\\nTask decomposition has been particularly effective for LLMs in applications such as conversational AI, where complex tasks require the model to understand context, intent, and nuance. By decomposing these tasks into smaller sub-tasks, LLM agents can develop more sophisticated understanding of human language and behavior, leading to improved performance in tasks such as dialogue management and sentiment analysis.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Do not provide any additional text besides the requested answer.\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL']) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e0d1da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "976c7ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, task decomposition for LLM (Large Language Model) agents can be done in three ways:\\n\\n1. With simple prompting, such as \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\".\\n2. Using task-specific instructions, e.g., \"Write a story outline.\" for writing a novel.\\n3. With human inputs.\\n\\nThis process involves breaking down complex tasks into smaller and simpler steps, allowing the LLM agent to plan ahead and utilize more test-time computation.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd635ae",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "97edcc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: langchain-ollama in ./.venv/lib/python3.12/site-packages (0.3.6)\n",
      "Requirement already satisfied: langchainhub in ./.venv/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.12/site-packages (1.0.17)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: youtube-transcript-api in ./.venv/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: pytube in ./.venv/lib/python3.12/site-packages (15.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.3.74)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.3.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.1 in ./.venv/lib/python3.12/site-packages (from langchain-ollama) (0.5.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (2.32.4.20250809)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.12/site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.12/site-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in ./.venv/lib/python3.12/site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-ollama langchainhub chromadb langchain youtube-transcript-api pytubefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18580d",
   "metadata": {},
   "source": [
    "### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11352d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasoruce: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "Based on the programming language the question is referring to, route it to the most relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f896379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasoruce='python_docs')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\"], \"speak in {language}])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f70cd9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasoruce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "17258b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasoruce.lower():\n",
    "        # Logic here\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasoruce.lower():\n",
    "        # Logic here\n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        # Logic here\n",
    "        return \"chain for golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7110a",
   "metadata": {},
   "source": [
    "### Semantic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b182239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole! One of the most fascinating and mysterious objects in the universe.\n",
      "\n",
      "A black hole is essentially a region in space where the gravitational pull is so strong that nothing, including light, can escape. It's formed when a massive star collapses in on itself and its gravity becomes so strong that it warps the fabric of spacetime around it.\n",
      "\n",
      "Imagine spacetime as a trampoline: if you place a heavy object, like a bowling ball, on the trampoline, it will warp and curve, creating a depression. Now imagine taking that bowling ball and making it infinitely dense and heavy – that's roughly what happens in a black hole. The gravity is so strong that it creates a boundary called the event horizon, which marks the point of no return.\n",
      "\n",
      "Once something crosses the event horizon, it's trapped by the black hole's gravity and can't escape. That's why black holes are invisible to us, as not even light can escape to reach our eyes.\n",
      "\n",
      "But here's the really cool thing about black holes: they're still a bit of a mystery, and scientists are still learning more about them!\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "WHen you don't know the answer you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions.\\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OllamaEmbeddings(model=os.environ[\"OLLAMA_EMBED_MODEL\"])\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt\n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59adccd",
   "metadata": {},
   "source": [
    "## Query Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1801a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix broken pytube\n",
    "import sys\n",
    "import pytubefix\n",
    "sys.modules[\"pytube\"] = pytubefix\n",
    "\n",
    "## Fix broken YoutubeTranscriptApi\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Patch: alias list_transcripts to list\n",
    "def patched_list_transcripts(video_id: str):\n",
    "    api = YouTubeTranscriptApi()\n",
    "    return api.list(video_id)\n",
    "\n",
    "YouTubeTranscriptApi.list_transcripts = staticmethod(patched_list_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "82695b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '2_Dtmpe9qaQ',\n",
       " 'title': \"Change your profile picture to clippy. I'm serious\",\n",
       " 'description': 'https://www.judiciary.senate.gov/committee-activity/hearings/a-time-for-truth-oversight-of-metas-foreign-relations-and-representations-to-the-united-states-congress\\n\\nhttps://futurism.com/facebook-beauty-targeted-ads\\nhttps://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp.\\nhttps://www.youtube.com/watch?v=RwSkwh3nWv8\\nhttps://www.youtube.com/watch?v=cayIOCg24bE\\nhttps://www.youtube.com/watch?v=tfAchfFXghc',\n",
       " 'view_count': 4020223,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/2_Dtmpe9qaQ/sddefault.jpg',\n",
       " 'publish_date': '2025-08-07 15:09:26',\n",
       " 'length': 472,\n",
       " 'author': 'Louis Rossmann'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "docs = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=2_Dtmpe9qaQ\", add_video_info=True).load()\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "54c50b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that coudl be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicity specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicity specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicity specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Latist publish date filter, exclusive. Only use if explicity specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicity specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicity specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c6919490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "system = \"\"\"You are and expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial video about a software library for building LLM-powered applications \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3a373355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch tutorial\n",
      "title_search: rag programming language tutorial\n",
      "latest_publish_date: 2022\n"
     ]
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d94e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_analyzer.invoke({\"question\": \"videos\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287077a",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb4d33",
   "metadata": {},
   "source": [
    "### Multi representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d977def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c34a0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1111a3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll provide a summary of the text in a concise format, highlighting the main points and key takeaways.\\n\\n**Summary**\\n\\nThe text discusses the limitations of Large Language Models (LLMs) in building autonomous agents. The authors highlight five common challenges:\\n\\n1. **Finite context length**: LLMs struggle with limited communication bandwidth, making it difficult to incorporate historical information and learn from past mistakes.\\n2. **Long-term planning and task decomposition**: LLMs find it challenging to adjust plans when faced with unexpected errors, making them less robust compared to humans.\\n3. **Reliability of natural language interface**: The reliability of model outputs is questionable, as LLMs may make formatting errors or exhibit rebellious behavior.\\n4. **Challenges in reasoning and acting**: LLMs struggle to synergize reasoning and acting, leading to limitations in their ability to reason and act effectively.\\n5. **Need for better prompting techniques**: Prompting techniques are crucial in enabling LLMs to understand the task and generate effective responses.\\n\\n**Key Takeaways**\\n\\n* LLMs require more advanced prompting techniques to overcome their limitations.\\n* Long-term planning and task decomposition are essential for building robust autonomous agents.\\n* The reliability of natural language interfaces is critical for successful agent deployment.\\n* Reasoning and acting capabilities are crucial for LLMs to perform effectively in complex tasks.\\n\\n**References**\\n\\nThe text provides a list of references, including research papers and articles on the topic of LLMs, autonomous agents, and prompt engineering.\",\n",
       " 'This is a comprehensive overview of various methods and techniques for improving the quality and reliability of human-generated data, particularly in the context of machine learning models. Here\\'s a summary of the key points:\\n\\n**Data Quality**\\n\\n1. **Human-AI collaboration**: Francis Galton\\'s \"Vox populi\" (1907) highlights the importance of human judgment in data annotation.\\n2. **Crowdsourcing**: Chris Callison-Burch\\'s work on Mechanical Turk (2009) and Rottger et al.\\'s two contrasting data annotation paradigms for subjective NLP tasks (2022) demonstrate the need for careful evaluation and assessment of crowd-sourced data.\\n3. **Data cascades**: Sambasivan et al.\\'s study on \"Everyone wants to do the model work, not the data work\" (CHI 2021) highlights the importance of understanding how data is propagated through models.\\n\\n**Dealing with Disagreements**\\n\\n1. **Majority vote**: Davani et al.\\'s work on dealing with disagreements in subjective annotations (ACL 2022) shows that relying solely on majority vote can lead to suboptimal results.\\n2. **Jury learning**: Gordon et al.\\'s jury learning approach (CHI 2022) integrates dissenting voices into machine learning models, improving performance and robustness.\\n\\n**Influence Functions**\\n\\n1. **Understanding black-box predictions**: Koh & Liang\\'s work on influence functions (ICML 2017) provides a framework for understanding and analyzing the behavior of deep neural networks.\\n2. **Studying large language model generalization**: Grosse et al.\\'s study on influence functions for large language models (arXiv 2023) explores the use of influence functions to improve model performance.\\n\\n**Noisy Data**\\n\\n1. **Noisy cross-validation**: Chen et al.\\'s work on noisy cross-validation (ICML 2019) proposes a method for evaluating and utilizing deep neural networks trained with noisy labels.\\n2. **Identifying mislabeled data**: Pleiss et al.\\'s approach using the area under the margin ranking (NeuriPS 2020) identifies mislabeled data by analyzing the gradient updates.\\n\\n**Example Forgetting**\\n\\n1. **An empirical study of example forgetting**: Toneva et al.\\'s work on example forgetting during deep neural network learning (ICLR 2019) demonstrates that examples can be safely removed without compromising model performance.\\n2. **Dataset cartography**: Swayamdipta et al.\\'s dataset cartography approach (EMNLP 2020) maps and diagnoses datasets with training dynamics, providing insights into data quality.\\n\\n**Miscellaneous**\\n\\n1. **Quality control in crowdsourcing**: Daniel et al.\\'s survey on quality attributes, assessment techniques, and assurance actions (ACM Computing Surveys 2018) highlights the importance of quality control in crowdsourcing.\\n2. **Thinking about high-quality human data**: Lilian Weng\\'s blog post (2024) emphasizes the need for careful consideration of human-generated data quality.\\n\\nOverall, this overview highlights the importance of addressing data quality issues and developing methods to improve the reliability and robustness of machine learning models.']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a878b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# vectorstore to index child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OllamaEmbeddings(model=os.environ[\"OLLAMA_EMBED_MODEL\"]))\n",
    "\n",
    "# Storage layer for parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key:doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a6b8b78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '85a6a23f-344c-46ae-ae3e-fefe41892a32'}, page_content=\"I'll provide a summary of the text in a concise format, highlighting the main points and key takeaways.\\n\\n**Summary**\\n\\nThe text discusses the limitations of Large Language Models (LLMs) in building autonomous agents. The authors highlight five common challenges:\\n\\n1. **Finite context length**: LLMs struggle with limited communication bandwidth, making it difficult to incorporate historical information and learn from past mistakes.\\n2. **Long-term planning and task decomposition**: LLMs find it challenging to adjust plans when faced with unexpected errors, making them less robust compared to humans.\\n3. **Reliability of natural language interface**: The reliability of model outputs is questionable, as LLMs may make formatting errors or exhibit rebellious behavior.\\n4. **Challenges in reasoning and acting**: LLMs struggle to synergize reasoning and acting, leading to limitations in their ability to reason and act effectively.\\n5. **Need for better prompting techniques**: Prompting techniques are crucial in enabling LLMs to understand the task and generate effective responses.\\n\\n**Key Takeaways**\\n\\n* LLMs require more advanced prompting techniques to overcome their limitations.\\n* Long-term planning and task decomposition are essential for building robust autonomous agents.\\n* The reliability of natural language interfaces is critical for successful agent deployment.\\n* Reasoning and acting capabilities are crucial for LLMs to perform effectively in complex tasks.\\n\\n**References**\\n\\nThe text provides a list of references, including research papers and articles on the topic of LLMs, autonomous agents, and prompt engineering.\"),\n",
       " Document(metadata={'doc_id': '1734c901-c858-4277-8842-3756e1f448da'}, page_content='This is a comprehensive overview of various methods and techniques for improving the quality and reliability of human-generated data, particularly in the context of machine learning models. Here\\'s a summary of the key points:\\n\\n**Data Quality**\\n\\n1. **Human-AI collaboration**: Francis Galton\\'s \"Vox populi\" (1907) highlights the importance of human judgment in data annotation.\\n2. **Crowdsourcing**: Chris Callison-Burch\\'s work on Mechanical Turk (2009) and Rottger et al.\\'s two contrasting data annotation paradigms for subjective NLP tasks (2022) demonstrate the need for careful evaluation and assessment of crowd-sourced data.\\n3. **Data cascades**: Sambasivan et al.\\'s study on \"Everyone wants to do the model work, not the data work\" (CHI 2021) highlights the importance of understanding how data is propagated through models.\\n\\n**Dealing with Disagreements**\\n\\n1. **Majority vote**: Davani et al.\\'s work on dealing with disagreements in subjective annotations (ACL 2022) shows that relying solely on majority vote can lead to suboptimal results.\\n2. **Jury learning**: Gordon et al.\\'s jury learning approach (CHI 2022) integrates dissenting voices into machine learning models, improving performance and robustness.\\n\\n**Influence Functions**\\n\\n1. **Understanding black-box predictions**: Koh & Liang\\'s work on influence functions (ICML 2017) provides a framework for understanding and analyzing the behavior of deep neural networks.\\n2. **Studying large language model generalization**: Grosse et al.\\'s study on influence functions for large language models (arXiv 2023) explores the use of influence functions to improve model performance.\\n\\n**Noisy Data**\\n\\n1. **Noisy cross-validation**: Chen et al.\\'s work on noisy cross-validation (ICML 2019) proposes a method for evaluating and utilizing deep neural networks trained with noisy labels.\\n2. **Identifying mislabeled data**: Pleiss et al.\\'s approach using the area under the margin ranking (NeuriPS 2020) identifies mislabeled data by analyzing the gradient updates.\\n\\n**Example Forgetting**\\n\\n1. **An empirical study of example forgetting**: Toneva et al.\\'s work on example forgetting during deep neural network learning (ICLR 2019) demonstrates that examples can be safely removed without compromising model performance.\\n2. **Dataset cartography**: Swayamdipta et al.\\'s dataset cartography approach (EMNLP 2020) maps and diagnoses datasets with training dynamics, providing insights into data quality.\\n\\n**Miscellaneous**\\n\\n1. **Quality control in crowdsourcing**: Daniel et al.\\'s survey on quality attributes, assessment techniques, and assurance actions (ACM Computing Surveys 2018) highlights the importance of quality control in crowdsourcing.\\n2. **Thinking about high-quality human data**: Lilian Weng\\'s blog post (2024) emphasizes the need for careful consideration of human-generated data quality.\\n\\nOverall, this overview highlights the importance of addressing data quality issues and developing methods to improve the reliability and robustness of machine learning models.')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "04b4b0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='6934df9c-ede8-46b1-a3ea-173538147d62', metadata={'doc_id': '85a6a23f-344c-46ae-ae3e-fefe41892a32'}, page_content=\"I'll provide a summary of the text in a concise format, highlighting the main points and key takeaways.\\n\\n**Summary**\\n\\nThe text discusses the limitations of Large Language Models (LLMs) in building autonomous agents. The authors highlight five common challenges:\\n\\n1. **Finite context length**: LLMs struggle with limited communication bandwidth, making it difficult to incorporate historical information and learn from past mistakes.\\n2. **Long-term planning and task decomposition**: LLMs find it challenging to adjust plans when faced with unexpected errors, making them less robust compared to humans.\\n3. **Reliability of natural language interface**: The reliability of model outputs is questionable, as LLMs may make formatting errors or exhibit rebellious behavior.\\n4. **Challenges in reasoning and acting**: LLMs struggle to synergize reasoning and acting, leading to limitations in their ability to reason and act effectively.\\n5. **Need for better prompting techniques**: Prompting techniques are crucial in enabling LLMs to understand the task and generate effective responses.\\n\\n**Key Takeaways**\\n\\n* LLMs require more advanced prompting techniques to overcome their limitations.\\n* Long-term planning and task decomposition are essential for building robust autonomous agents.\\n* The reliability of natural language interfaces is critical for successful agent deployment.\\n* Reasoning and acting capabilities are crucial for LLMs to perform effectively in complex tasks.\\n\\n**References**\\n\\nThe text provides a list of references, including research papers and articles on the topic of LLMs, autonomous agents, and prompt engineering.\")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query, k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\"\n",
      "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\"\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(query, n_resuls=1)\n",
    "print(repr(retrieved_docs[0].page_content[0:100]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d657d27",
   "metadata": {},
   "source": [
    "### RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0260bfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.2 kiwisolver-1.4.9 matplotlib-3.10.5 pillow-11.3.0 pyparsing-3.2.3\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aa1c5360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/how_to/output_parser_structured/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/how_to/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, * docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the historgram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# diplay\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7052891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
