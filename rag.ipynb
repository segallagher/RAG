{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6e6dab",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6dc3e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: langchain-ollama in ./.venv/lib/python3.12/site-packages (0.3.6)\n",
      "Requirement already satisfied: ollama in ./.venv/lib/python3.12/site-packages (0.5.3)\n",
      "Requirement already satisfied: langchainhub in ./.venv/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.12/site-packages (1.0.17)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.3.27)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.3.74)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.4.14)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.3.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: httpx>=0.27 in ./.venv/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in ./.venv/lib/python3.12/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (2.32.4.20250809)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.12/site-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./.venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.5 bs4-0.0.2 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-ollama ollama langchainhub chromadb langchain bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5bac35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9c2ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OLLAMA_HOST=http://172.25.208.1:11434\n"
     ]
    }
   ],
   "source": [
    "! source get_host.sh && echo $OLLAMA_HOST > tmp_env.txt\n",
    "with open('tmp_env.txt') as f:\n",
    "    OLLAMA_HOST = f.read().strip()\n",
    "%env OLLAMA_HOST=$OLLAMA_HOST\n",
    "import os\n",
    "os.remove(\"tmp_env.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b6a1f",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc52b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c34c579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name:str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaa2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text embedding models\n",
    "# https://ollama.com/blog/embedding-models\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# embd = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embd = OllamaEmbeddings(model=os.environ[\"OLLAMA_EMBED_MODEL\"])\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ec7c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.2761092814708602\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27edab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import os\n",
    "\n",
    "os.environ['USER_AGENT'] = 'myagent'\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f0ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d65426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstores\n",
    "# index\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(model=os.environ[\"OLLAMA_LLM_MODEL\"])\n",
    "                                   )\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9bb737",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(model=os.environ[\"OLLAMA_EMBED_MODEL\"])\n",
    "                                   )\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2708360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98f31091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25a028",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88e72e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=os.environ.get(\"OLLAMA_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"OLLAMA_HOST\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "815a76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d593e36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition is the process of breaking down a complicated task into smaller and simpler steps. It involves using techniques such as Chain of Thought (CoT), Tree of Thoughts, or other methods to analyze the problem and generate multiple possible solutions at each step. The goal is to make the task more manageable and understandable for an agent (e.g., a language model) by transforming complex tasks into multiple smaller ones.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-08-26T18:10:17.0767458Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1250924700, 'load_duration': 58223700, 'prompt_eval_count': 332, 'prompt_eval_duration': 22225900, 'eval_count': 82, 'eval_duration': 1169818900, 'model_name': 'llama3.2:3b'}, id='run--90d69d2b-33ca-4052-95ef-166e61672b1d-0', usage_metadata={'input_tokens': 332, 'output_tokens': 82, 'total_tokens': 414})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke\n",
    "chain.invoke({\"context\": docs, \"question\": \"What is task decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c86b248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt hub\n",
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e98f61",
   "metadata": {},
   "source": [
    "Rag Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74309b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, Task Decomposition refers to breaking down complex tasks into smaller and simpler steps, allowing an agent (or a model) to plan ahead and tackle hard tasks more efficiently. This technique is also known as Chain of Thought (CoT), which involves instructing the model to \"think step by step\" to decompose the task.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404ae99",
   "metadata": {},
   "source": [
    "## Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc091f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# mutli-query\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help the user\n",
    "overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by only one newline character. Only provide the requested\n",
    "perspectives without any additional text. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOllama(temperature=0, model=os.environ[\"OLLAMA_LLM_MODEL\"])\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [line for line in x.split('\\n') if line.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc770e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2d0b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals. This process enables efficient handling of complex tasks.\\n\\nThere are three ways to perform task decomposition:\\n\\n1. Using simple prompting with instructions like \"Steps for XYZ. 1.\", or \"What are the subgoals for achieving XYZ?\".\\n2. Using task-specific instructions, such as \"Write a story outline.\" for writing a novel.\\n3. With human inputs.\\n\\nTask decomposition is used in conjunction with other techniques, such as Chain of Thought (CoT) and Tree of Thoughts (Yao et al. 2023), to help the LLM agent plan ahead and tackle complex tasks.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f49a7e",
   "metadata": {},
   "source": [
    "## RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f44dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search quesries based on a single input entry. \\n Output (4 queries:)\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82f49352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOllama(temperature=0, model=os.environ[\"OLLAMA_LLM_MODEL\"])\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [line for line in x.split('\\n') if line.strip()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01ba27e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used inthe RRF formula \"\"\"\n",
    "    \n",
    "    # initialize a dictionary to hold fused scores for each unique deployment\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be seriealsed to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_score dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # retrieve the current score of the document\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "    \n",
    "    # Sort the docuents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f11a0824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, task decomposition for LLM (Large Language Model) agents involves breaking down large tasks into smaller, manageable subgoals. This allows the agent to efficiently handle complex tasks.\\n\\nThere are three ways to do task decomposition:\\n\\n1. Using simple prompting: e.g., \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\"\\n2. Using task-specific instructions: e.g., \"Write a story outline.\"\\n3. With human inputs.\\n\\nTask decomposition is one of the key components of a LLM-powered autonomous agent system, along with reflection and refinement.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2d41c",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b179b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Do not provide any other text, only provide the generated search queries \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d6837de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the role of a large language model in an autonomous agent system?',\n",
       " '2. How do reinforcement learning algorithms contribute to the development of autonomous agents powered by LLMs?',\n",
       " '3. What are the key differences between LLM-powered and traditional machine learning approaches for autonomous decision-making?']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() |  (lambda x: [line for line in x.split('\\n') if line.strip()]))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9636931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n -- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0220218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n --- \\nQuestion: 1. What is the role of a large language model in an autonomous agent system?\\nAnswer: Based on the provided context, a large language model (LLM) plays a crucial role as the brain of an autonomous agent system. In this setup, the LLM functions as the primary controller, complemented by several key components:\\n\\n1. Planning: The LLM breaks down complex tasks into smaller subgoals, enabling efficient handling of intricate tasks.\\n2. Reflection and refinement: The LLM can perform self-criticism and self-reflection over past actions, learning from mistakes and refining them for future steps to improve the quality of final results.\\n\\nIn summary, the role of a large language model in an autonomous agent system is to serve as the intelligent core controller that enables planning, reflection, and refinement, ultimately driving the agent's decision-making process.\\n --- \\nQuestion: 2. How do reinforcement learning algorithms contribute to the development of autonomous agents powered by LLMs?\\nAnswer: Based on the provided context, reinforcement learning algorithms play a crucial role in the development of autonomous agents powered by large language models (LLMs). Here's how:\\n\\n1. **Learning from interactions**: Reinforcement learning algorithms enable the agent to learn from its interactions with the environment, receiving rewards or penalties for its actions. This process helps the LLM refine its decision-making and improve its performance over time.\\n2. **Policy optimization**: Reinforcement learning algorithms can optimize the policy of the LLM, which is the mapping between states and actions. By iteratively updating the policy based on the agent's experiences, reinforcement learning algorithms help the LLM make more informed decisions in complex environments.\\n3. **Value function estimation**: Reinforcement learning algorithms also enable the estimation of value functions, which represent the expected utility or reward of taking a particular action in a given state. This information is crucial for the LLM to make informed decisions and balance competing goals.\\n4. **Exploration-exploitation trade-off**: Reinforcement learning algorithms help the agent navigate the exploration-exploitation trade-off, balancing the need to explore new actions and states with the need to exploit known opportunities for reward.\\n\\nIn the context of LLM-powered autonomous agents, reinforcement learning algorithms can be used to:\\n\\n* Train the LLM on tasks such as planning, reflection, and refinement\\n* Optimize the policy of the LLM to balance competing goals and optimize performance\\n* Estimate value functions to inform decision-making\\n* Enable the agent to learn from its interactions with the environment and adapt to changing conditions\\n\\nBy leveraging reinforcement learning algorithms, developers can create more sophisticated autonomous agents powered by LLMs, capable of learning from their environments and improving their performance over time.\\n --- \\nQuestion: 3. What are the key differences between LLM-powered and traditional machine learning approaches for autonomous decision-making?\\nAnswer: Based on the provided context and background information, here are the key differences between LLM-powered and traditional machine learning approaches for autonomous decision-making:\\n\\n**Key Differences:**\\n\\n1. **Planning and Subgoal Decomposition**: LLM-powered agents use planning as a key component to break down complex tasks into smaller subgoals, enabling efficient handling of intricate tasks. In contrast, traditional machine learning approaches may rely on more simplistic methods such as rule-based systems or heuristic search.\\n2. **Reflection and Refinement**: LLM-powered agents can perform self-criticism and self-reflection over past actions, learning from mistakes and refining them for future steps to improve the quality of final results. Traditional machine learning approaches typically do not have this level of self-awareness and reflection capabilities.\\n3. **Learning from Interactions**: LLM-powered agents learn from their interactions with the environment using reinforcement learning algorithms, which enable the agent to receive rewards or penalties for its actions. Traditional machine learning approaches may rely on supervised learning or unsupervised learning methods, which do not involve direct interaction with the environment.\\n4. **Policy Optimization and Value Function Estimation**: LLM-powered agents use reinforcement learning algorithms to optimize their policy and estimate value functions, which represent the expected utility or reward of taking a particular action in a given state. Traditional machine learning approaches may rely on more simplistic methods such as gradient descent optimization or heuristic search.\\n5. **Exploration-Exploitation Trade-off**: LLM-powered agents use reinforcement learning algorithms to navigate the exploration-exploitation trade-off, balancing the need to explore new actions and states with the need to exploit known opportunities for reward. Traditional machine learning approaches may not have this level of nuance in their decision-making processes.\\n\\n**Traditional Machine Learning Approaches:**\\n\\nIn contrast, traditional machine learning approaches for autonomous decision-making often rely on more simplistic methods such as:\\n\\n* Rule-based systems\\n* Heuristic search\\n* Supervised learning\\n* Unsupervised learning\\n* Gradient descent optimization\\n\\nThese approaches may not have the same level of self-awareness, reflection capabilities, or ability to learn from interactions with the environment as LLM-powered agents.\\n\\nIn summary, LLM-powered autonomous decision-making approaches offer several key advantages over traditional machine learning approaches, including planning and subgoal decomposition, reflection and refinement, learning from interactions, policy optimization and value function estimation, and exploration-exploitation trade-off.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm \n",
    "llm = ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "\n",
    "# Generate QA pairs\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pair\n",
    "\n",
    "q_a_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b193e",
   "metadata": {},
   "source": [
    "## Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e7dbc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Could the members of the police perform lawful arrests?', 'output': 'What can the members of the police do?'}, {'input': \"Jan Sindel's was born in what country?\", 'output': \"What is Jan Sindel's personal history?\"}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of the police perform lawful arrests?\",\n",
    "        \"output\": \"What can the members of the police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"What is Jan Sindel's personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to examples messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. You must respond with only one step-back question with no added text. Here are a few examples:\"\"\"\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "20c5ff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do large language models process complex tasks?'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL']) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e58dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial component of LLM-powered autonomous agent systems, enabling the efficient handling of complex tasks by breaking them down into smaller, manageable subgoals. In the context of LLM agents, task decomposition involves instructing the model to \"think step by step\" and utilize more test-time computation to decompose hard tasks into simpler steps.\\n\\nThere are several techniques used for task decomposition in LLM-powered agent systems:\\n\\n1. **Chain of Thought (CoT)**: This technique, introduced by Wei et al. (2022), involves instructing the model to \"think step by step\" and generate a chain of thoughts that decompose the complex task into smaller steps. CoT transforms big tasks into multiple manageable tasks and sheds light on the model\\'s thinking process.\\n2. **Tree of Thoughts (ToT)**: This technique, introduced by Yao et al. (2023), extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be performed using breadth-first search (BFS) or depth-first search (DFS) with each state evaluated by a classifier or majority vote.\\n3. **Simple prompting**: LLMs can also perform task decomposition through simple prompting, such as \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\".\\n4. **Task-specific instructions**: Task-specific instructions, such as \"Write a story outline,\" can be used to guide the model\\'s task decomposition process.\\n5. **Human inputs**: Human inputs can also be used to provide guidance on task decomposition, allowing the model to learn from human feedback and improve its performance over time.\\n\\nThe goal of task decomposition in LLM-powered agent systems is to enable the model to break down complex tasks into smaller, more manageable subgoals, which can then be tackled one by one. This approach allows the model to efficiently handle complex tasks and improve the quality of its final results through self-reflection and refinement.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "# Response prompt\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL'])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0da1d",
   "metadata": {},
   "source": [
    "## HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7837812a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for Large Language Model (LLM) agents refers to the process of breaking down complex tasks into smaller, more manageable sub-tasks that can be executed by the model in a hierarchical manner. This approach enables the development of more robust and generalizable LLMs by allowing them to focus on specific aspects of the task at hand, rather than attempting to tackle the entire task simultaneously.\\n\\nBy decomposing tasks into sub-tasks, LLM agents can leverage their strengths in processing and generating text to address specific components of the task, such as question answering, text classification, or language translation. This hierarchical approach also enables the model to learn more nuanced representations of the task and its sub-tasks, leading to improved performance and generalizability.\\n\\nTask decomposition has been particularly effective for LLMs in applications such as conversational AI, where complex tasks require the model to understand context, intent, and nuance. By decomposing these tasks into smaller sub-tasks, LLM agents can develop more sophisticated understanding of human language and behavior, leading to improved performance in tasks such as dialogue management and sentiment analysis.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Do not provide any additional text besides the requested answer.\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOllama(temperature=0, model=os.environ['OLLAMA_LLM_MODEL']) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e0d1da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "976c7ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, task decomposition for LLM (Large Language Model) agents can be done in three ways:\\n\\n1. With simple prompting, such as \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\".\\n2. Using task-specific instructions, e.g., \"Write a story outline.\" for writing a novel.\\n3. With human inputs.\\n\\nThis process involves breaking down complex tasks into smaller and simpler steps, allowing the LLM agent to plan ahead and utilize more test-time computation.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97edcc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
